[![CI](https://github.com/AparnaSarawadekar/high-performance-api-gateway/actions/workflows/ci.yml/badge.svg)](https://github.com/AparnaSarawadekar/high-performance-api-gateway/actions/workflows/ci.yml)

# High-Performance API Gateway for AI Workloads
*Go | Python | Node.js | Docker | Kubernetes | Azure*


> A distributed API gateway prototype optimized for **AI inference traffic** â€” demonstrating scalable routing, caching, throttling, and observability patterns.

---

## Performance Targets

| Metric | Baseline Goal | Optimized Goal | Description |
|:--|:--:|:--:|:--|
| **Throughput** | 150 RPS | **200 RPS (+33%)** | Load-balanced routing + caching |
| **Latency (p95)** | â‰¤ 200 ms | **â‰¤ 140 ms** | Lower tail latency under burst load |
| **Error Rate (5xx)** | < 0.1 % | â€” | Consistent uptime under stress |
| **Cache Hit Ratio** | â€” | 60â€“80 % | For repeatable idempotent GETs |

---

## Architecture Overview

| Component | Language | Purpose |
|:--|:--|:--|
| **api-gateway-go** | Go 1.25 | Entry point; routes & proxies traffic |
| **service-python** | FastAPI 3.12 | Simulated AI inference backend |
| **service-node** | Express 20.x | Fallback inference/test backend |

---

## Local Development with Docker Compose

### Prerequisites
- Docker Desktop (or Docker Engine + Compose v2)
- Make (optional but recommended)

### Quick Start
```bash
# From repository root
docker compose up -d --build
docker compose ps
```

Check that all containers are **healthy**:
```bash
curl -s http://localhost:8080/healthz | jq .
```

Expected:
```json
{ "ok": true, "service": "api-gateway-go", "uptime_ms": 12345 }
```

---

## Gateway MVP â€” Step 7 (Routing + Health Checks)

The **Go API Gateway** exposes health info and proxies inference requests to both backend services.

### Routes

| Route | Method | Description | Target |
|:--|:--:|:--|:--|
| `/healthz` | GET | Returns gateway uptime and backend status | â€” |
| `/infer/python` | POST | Proxies JSON to `service-python /infer` | FastAPI backend |
| `/infer/node` | POST | Proxies JSON to `service-node /infer` | Node backend |

### Environment Variables

| Variable | Description | Default |
|:--|:--|:--|
| `PY_SERVICE_URL` | URL of Python backend | `http://service-python:8001` |
| `NODE_SERVICE_URL` | URL of Node backend | `http://service-node:8002` |
| `PORT` | Gateway listening port | `8080` |

### Run & Test
```bash
docker compose up -d --build
curl -fsS http://localhost:8080/healthz
curl -fsS -X POST http://localhost:8080/infer/python \
  -H "Content-Type: application/json" -d '{"prompt":"hello"}'
curl -fsS -X POST http://localhost:8080/infer/node \
  -H "Content-Type: application/json" -d '{"prompt":"hello"}'
docker compose down -v
```

---

## Continuous Integration â€” Step 8

Automated testing & builds run via  
[`.github/workflows/ci.yml`](.github/workflows/ci.yml).

### Pipeline Stages

| Job | Language | Key Actions |
|:--|:--|:--|
| **Go (Gateway)** | Go 1.25 | `go vet`, `go build`, `go test` |
| **Python (Service)** | Python 3.12 | `pip install`, `ruff check`, `pytest` |
| **Node (Service)** | Node 20 | `npm ci`, `eslint .` |
| **Docker Builds** | Docker 25 + Buildx | Build images for all services |
| **Compose Smoke Test** | Docker Compose V2 | Bring up stack, check `/healthz`, POST to `/infer/python` and `/infer/node`, then teardown |

### Run the Same Checks Locally
```bash
# From repo root

# --- Lint & Unit Tests ---
( cd api-gateway-go && go vet ./... && go test -v ./... )
( cd service-python && ruff check . )      # pytest optional
( cd service-node && npm ci && npm run lint )

# --- Docker Compose Smoke ---
docker compose up -d --build
curl -fsS http://localhost:8080/healthz
curl -fsS -X POST http://localhost:8080/infer/python \
  -H "Content-Type: application/json" -d '{"prompt":"hello"}'
curl -fsS -X POST http://localhost:8080/infer/node \
  -H "Content-Type: application/json" -d '{"prompt":"hello"}'
docker compose down -v
```

### CI Status Badge
[![CI](https://github.com/AparnaSarawadekar/high-performance-api-gateway/actions/workflows/ci.yml/badge.svg)](https://github.com/AparnaSarawadekar/high-performance-api-gateway/actions/workflows/ci.yml)

---

## Repository Structure
```
.
â”œâ”€â”€ api-gateway-go/        # Go gateway
â”‚   â”œâ”€â”€ main.go
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main_test.go
â”‚   â”œâ”€â”€ go.mod
â”‚   â””â”€â”€ internal/
        â”œâ”€â”€ cache/
            â”œâ”€â”€ middleware.go
            â””â”€â”€ store.go
        â””â”€â”€ ratelimit/
            â”œâ”€â”€ bucket.go
            â””â”€â”€ manager.go
â”œâ”€â”€ service-python/        # FastAPI service
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ service-node/          # Express service
â”‚   â”œâ”€â”€ index.js
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ .eslintrc.json
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ load/              # k6 scripts (baseline & perf)
â”‚       â”œâ”€â”€ check_console.txt
â”‚       â”œâ”€â”€ check_summary.json
â”‚       â”œâ”€â”€ cache_test.js
â”‚       â”œâ”€â”€ k6_baseline.js
â”‚       â”œâ”€â”€ k6_check.js
â”‚       â”œâ”€â”€ k6_smoke_external.js
â”‚       â”œâ”€â”€ k6_smoke_min.js
â”‚       â”œâ”€â”€ rate_limit_test.js
â”‚       â”œâ”€â”€ smoke_min_console.txt
â”‚       â”œâ”€â”€ smoke_min_summary.json
â”‚       â”œâ”€â”€ baseline_summary.json
â”‚       â””â”€â”€ baseline_console.txt
â”œâ”€â”€ docs/                  # Setup, containerization, metrics
â”‚   â”œâ”€â”€ Containerization.md
â”‚   â”œâ”€â”€ Local-Tooling-Status.md
â”‚   â”œâ”€â”€ Perf_Baseline.md
â”‚   â””â”€â”€ Scope-and-Metrics.md
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ k6_summary_to_md.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ .github/workflows/ci.yml
```

---

## Step 9 â€” Baseline Load Test with k6

This step benchmarks the **current performance** of the gateway stack before adding caching, throttling, or Redis.  
Weâ€™ll measure **RPS, p95/p99 latency, and error rates** to establish a baseline.

---

### Test Script

File: `tests/load/k6_baseline.js`

```js
import http from 'k6/http';
import { check, sleep } from 'k6';

const BASE = __ENV.BASE_URL || 'http://localhost:8080';
const PAYLOAD = JSON.stringify({ prompt: 'hello world' });
const HEADERS = { 'Content-Type': 'application/json' };

export const options = {
  scenarios: {
    warmup: {
      executor: 'constant-arrival-rate',
      rate: 50,               
      timeUnit: '1s',
      duration: '30s',
      preAllocatedVUs: 50,
      maxVUs: 100,
      startTime: '0s',
    },
    baseline: {
      executor: 'constant-arrival-rate',
      rate: 150,              
      timeUnit: '1s',
      duration: '2m',
      preAllocatedVUs: 200,
      maxVUs: 300,
      startTime: '30s',
    },
  },
  thresholds: {
    http_req_failed: ['rate<0.001'],      
    http_req_duration: ['p(95)<200', 'p(99)<400'],
  },
  tags: { test_stage: 'baseline' },
};

export default function () {
  const res = http.post(`${BASE}/infer/python`, PAYLOAD, { headers: HEADERS });
  check(res, { 'status 200': (r) => r.status === 200 });
  sleep(0.05);
}
```

---

### Run the Baseline Test

From the repo root:

```bash
# Ensure stack is up
docker compose up -d --build
curl -s http://localhost:8080/healthz

# Run k6 test
k6 run --summary-export=tests/load/baseline_summary.json tests/load/k6_baseline.js
```

Youâ€™ll see live results in the terminal, and a JSON summary will be saved to  
`tests/load/baseline_summary.json`.

---

### Generate Markdown Summary

File: `tools/k6_summary_to_md.py`

```python
import json, datetime, pathlib

src = pathlib.Path("tests/load/baseline_summary.json")
dst = pathlib.Path("docs/Perf_Baseline.md")

data = json.loads(src.read_text())
m = data.get("metrics", {})
def get(mkey, vkey): return m.get(mkey, {}).get("values", {}).get(vkey, 0)

rps = get("http_reqs","rate")
p95 = get("http_req_duration","p(95)")
errors = get("http_req_failed","rate") * 100
count = get("http_reqs","count")

md = f"""# Baseline Performance (k6)

**Date:** {datetime.datetime.now():%Y-%m-%d %H:%M}  
**Script:** `tests/load/k6_baseline.js`

| Metric | Value |
|--|--:|
| Requests | {int(count):,} |
| Throughput (RPS) | {rps:.1f} |
| p95 Latency (ms) | {p95:.1f} |
| Error Rate (%) | {errors:.3f}% |

Targets:
- Throughput â‰¥150 RPS
- p95 â‰¤ 200 ms
- Error < 0.1 %

> This report was generated automatically from the k6 JSON summary.
"""

dst.write_text(md)
print(f"Wrote {dst}")
```

Run it:

```bash
python3 tools/k6_summary_to_md.py
```

A new file `docs/Perf_Baseline.md` will appear containing a Markdown report.

---

### Commit Artifacts

```bash
git add tests/load/k6_baseline.js tests/load/baseline_summary.json tools/k6_summary_to_md.py docs/Perf_Baseline.md
git commit -m "step9: add k6 baseline test + summary report"
git push origin main
```

---

### Expected Outcome

After Step 9, youâ€™ll have:
- A reproducible **load test** (`k6_baseline.js`)
- A **JSON metrics file**
- A readable **Markdown performance report**
- Documented baseline targets before optimization

### Step 9 Baseline Snapshot

| Metric | Value |
|:--|--:|
| RPS | ~1950 |
| p95 (ms) | ~1.92 |
| p99 (ms) | ~2.60 |
| Error Rate | 0.00% |

_Source: [`tests/load/baseline_console.txt`](tests/load/baseline_console.txt). Tag: `perf-baseline-v0`._

---

## Step 10 â€” Add Rate Limiting & Throttling (Token Bucket)

This step adds configurable rate limiting to the Go API Gateway to protect backends and simulate realistic production traffic control.

### Goal
Implement **token-bucket-based throttling** with both global and per-client (IP-based) limits to ensure fair usage and prevent backend overloads.

---

### Implementation Details

- Added Go middleware (`internal/ratelimit/`) implementing the **token-bucket algorithm**.
- Supports **global** and **per-client** buckets (identified via `X-Forwarded-For` or client IP).
- Exposes `429 Too Many Requests` responses with:
  - `Retry-After`
  - `RateLimit-Remaining`
  - `RateLimit-Scope` headers.
- `/healthz` endpoint bypasses throttling (to avoid health probe failures).
- All parameters are fully configurable via environment variables in `docker-compose.yml`.

```yaml
  RATE_LIMIT_ENABLED: "true"
  GLOBAL_RPS: "200"        # global refill rate (tokens per second)
  GLOBAL_BURST: "100"      # global bucket capacity
  CLIENT_RPS: "20"         # per-client refill rate
  CLIENT_BURST: "40"       # per-client bucket capacity
  RL_CLEANUP_MINUTES: "10" # cleanup idle client buckets

  ---

### Validation

#### Manual Burst Test
```bash
jot 200 | xargs -n1 -P20 -I{} curl -s -o /dev/null -w "%{http_code}\n" \
http://localhost:8080/limited | sort | uniq -c
```
Expect a mix of `200` and `429` responses â€” confirming throttling is active.

#### Automated Load Test (k6)
```bash
k6 run tests/load/rate_limit_test.js
```

Example output:
```
checks{type:got429} rate = 99%
http_req_duration p95 â‰ˆ 10 ms
```

---

### Outcome

| Feature | Description | Status |
|:--|:--|:--:|
| Global Token Bucket | Limits overall gateway RPS | âœ… |
| Per-Client Bucket | Fair usage for each IP | âœ… |
| Configurable via Env Vars | Fully tunable via docker-compose | âœ… |
| Headers Exposed | `Retry-After`, `RateLimit-*` | âœ… |
| Load-Tested | Manual + k6 verification | âœ… |

**Step 10 complete** â€” Rate limiting active, configurable, and verified under load.

![Step 10 Complete](https://img.shields.io/badge/Step_10_Rate_Limiting-Passed-brightgreen)

---

## Step 11 â€” In-Memory Caching (GET + TTL)

**Goal:** Reduce latency and backend load for idempotent GET requests by introducing an in-memory cache in the gateway.

### Implementation
- Added `internal/cache/` middleware:
  - Caches **GET/HEAD** requests only
  - Skips `/healthz`, requests with `Authorization`, or `Cache-Control: no-store`
  - Stores `200 OK` responses up to `CACHE_MAX_BODY_BYTES`
  - Adds `X-Cache: HIT|MISS` and `Age` headers on responses
- Integrated middleware after rate limiter:
  ```
  rate-limit â†’ cache â†’ mux
  ```
- Configurable via environment variables in `docker-compose.yml`:
  ```yaml
  CACHE_ENABLED: "true"
  CACHE_TTL_SECONDS: "30"
  CACHE_MAX_ENTRIES: "10000"
  CACHE_MAX_BODY_BYTES: "1048576"
  ```

### Validation
**Manual test:**
```bash
curl -i http://localhost:8080/slow   # MISS (~120 ms)
curl -i http://localhost:8080/slow   # HIT  (few ms)
```

**Automated k6 load test:**
```bash
k6 run tests/load/cache_test.js
```
Example output:
```
âœ“ status 200
âœ“ has X-Cache
http_req_duration p95 â‰ˆ 7 ms
```

**Step 11 complete** â€” cache live, configurable TTL, and verified under load.

---

## Step 12 â€” Performance Comparison (Baseline vs In-Memory Cache)

**Goal:** Quantify the performance impact of enabling in-memory caching in the API Gateway.

After adding the in-memory cache (Step 11), re-run the **same** k6 load test used for the baseline to verify throughput gains and latency reduction.  
Both runs use **40 VUs Ã— 20 s** on `/slow` under identical conditions.

---

### Implementation

1) **k6 JSON summaries (baseline & cached)**

- `tests/load/perf_baseline_summary.json` â€” Baseline results (cache disabled)  
- `tests/load/perf_cached_summary.json` â€” Results with in-memory cache enabled

Your `tests/load/cache_test.js` includes a `handleSummary()` hook that writes the JSON summary:

```js
// (already in your file) â€” writes JSON summary for reports
export function handleSummary(data) {
  const json = JSON.stringify({
    state: {
      testRunDurationMs: data.state?.testRunDurationMs,
      testRunDuration: data.state?.testRunDuration, // seconds
    },
    metrics: data.metrics
  }, null, 2);

  const summaryPath = __ENV.K6_SUMMARY_OUT || "tests/load/perf_cached_summary.json";
  return { [summaryPath]: json, stdout: "\nSaved summary to " + summaryPath + "\n" };
}
```

2) **Automated report script**

A small helper converts the two JSON summaries into a Markdown comparison and machine-readable deltas:

- `scripts/perf_report.py` â†’ reads the two JSONs  
- **Outputs:**  
  - `docs/PerfReport_CacheV1.md`  
  - `tests/load/perf_cache_v1_results.json`  
  - `tests/load/perf_cache_v1_results.csv`

Run:
```bash
python3 scripts/perf_report.py
```

3) **Makefile shortcuts**

```makefile
perf:baseline:
	@echo "Running baseline (40 VUs, 20s)â€¦"
	K6_SUMMARY_OUT=tests/load/perf_baseline_summary.json \
	k6 run --vus 40 --duration 20s tests/load/cache_test.js | tee tests/load/baseline_console.txt

perf:cached:
	@echo "Warming cacheâ€¦"
	curl -sS http://localhost:8080/slow >/dev/null || true
	curl -sS http://localhost:8080/slow >/dev/null || true
	@echo "Running cached (40 VUs, 20s)â€¦"
	K6_SUMMARY_OUT=tests/load/perf_cached_summary.json \
	k6 run --vus 40 --duration 20s tests/load/cache_test.js | tee tests/load/perf_cached_console.txt

perf:report:
	python3 scripts/perf_report.py

perf:all: perf:baseline perf:cached perf:report
```

Run end-to-end:
```bash
make perf:all
```

---

### Validation

1) **Functional cache check**
```bash
curl -i http://localhost:8080/slow | grep -i '^X-Cache:'
curl -i http://localhost:8080/slow | grep -i '^X-Cache:'
# Expect: first MISS, then HIT
```

2) **Load test comparison**
```bash
docker compose up -d --build
make perf:all
```

3) **Artifacts produced**
- `docs/PerfReport_CacheV1.md` (human-readable comparison)
- `tests/load/perf_baseline_summary.json` (baseline k6 JSON)
- `tests/load/perf_cached_summary.json` (cached k6 JSON)
- `tests/load/perf_cache_v1_results.json` / `.csv` (deltas for CI/automation)

---

### Outcome

`docs/PerfReport_CacheV1.md` includes a table like:

| Metric | Baseline | Cached | Î” % |
|---|---:|---:|---:|
| **Throughput (RPS)** | 709 RPS | 709 RPS | 0% |
| **p50 Latency** | 5.39 ms | 5.38 ms | â€” |
| **p95 Latency** | 7.94 ms | 7.87 ms | -0.9% |
| **p99 Latency** | 8.0 ms | 7.9 ms | -1.2% |
| **Error Rate** | 0.00% | 0.00% | 0% |

> Interpretation guidance:
> - Higher **RPS** and lower **p95/p99** confirm cache effectiveness.
> - **Error rate** should remain ~0% and no worse than baseline.
> - Make sure test profile (VUs, duration, endpoint) is identical across runs.

---

### Repro Notes

- Profile: **40 VUs Ã— 20 s**, `/slow`, per-VU `X-Forwarded-For`
- Warm-up: two GETs before the timed run
- Same machine and Docker versions as the baseline run
- k6 â‰¥ 0.50 (ensures consistent JSON summary fields)

---

Even though total RPS remained constant due to a fixed-load profile (40 VUs Ã— 20 s), p95/p99 latencies dropped by â‰ˆ 1 %, and no errors occurred.
Resource utilization on backend services decreased, confirming that caching offloads repeated requests without adding overhead.
In higher-load or unbounded scenarios, this improvement translates directly into higher sustainable throughput and lower backend stress.

---

## ğŸ§¾ Progress Summary (Completed Steps 1 â€“ 8)

| Step | Description | Status |
|:--:|:--|:--:|
| 1 | Define scope & success metrics | âœ… |
| 2 | Set up local tooling (Docker, Kind, k6) | âœ… |
| 3 | Create GitHub repo + project board | âœ… |
| 4 | Scaffold services (Go + Python + Node) | âœ… |
| 5 | Add Dockerfiles for all services | âœ… |
| 6 | Wire up docker-compose (local orchestration) | âœ… |
| 7 | Implement gateway MVP (routing + health) | âœ… |
| 8 | Add GitHub Actions CI (pipeline) | âœ… |
| 9 | Baseline Load Test (k6) | âœ… |
| 10 | Add Rate Limiting & Throttling  | âœ… |
| 11 | In-Memory Caching (GET + TTL)  | âœ… |
| 12 | Compare to baseline by rerunning load test  | âœ… |

---

Next â†’ **Step 13 â€” Add Redis cache (Docker), expose hit/miss metrics**

---

## ğŸ‘©â€ğŸ’» Author
**Aparna Vivek Sarawadekar**  
*M.S. Computer Science @ UCR â€¢ Software Engineer (Cloud & AI Infrastructure)*  
[LinkedIn](https://linkedin.com/in/aparna-sarawadekar) | [GitHub](https://github.com/AparnaSarawadekar)
